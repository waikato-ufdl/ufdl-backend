{
  "name": "tensorflow_objectdetection_1_14_mask_rcnn_inception_v2_coco-train",
  "version": 1,
  "description": "Building Mask R-CNN Inception v2 models using ObjectDetection API for Tensorflow 1.14\nhttp://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz",
  "scope": "public",
  "domain": "od",
  "licence": "Apache 2.0",
  "specific": {
    "job_type": "Train<Domain<'Object Detection'>, Framework<'tensorflow', '1.14'>>",
    "executor_class": "ufdl.joblauncher.objdet.tensorflow.ObjectDetectionTrain_TF_1_14",
    "required_packages": "git+https://github.com/waikato-ufdl/ufdl-job-launcher-plugins.git",
    "parameters": {
      "dataset_options": {
        "types": ["Array<str>", "str"],
        "default": [
          "to-tf-od",
          "-o", "data.tfrecords",
          "-p", "labels.pbtxt",
          "--split-names", "train", "test", "val",
          "--split-ratios", "70", "15", "15"
        ],
        "help": "Options to the dataset input"
      },
      "docker_image": {
        "types": [
          "DockerImage<Domain<'Object Detection'>, Framework<'tensorflow', '1.14'>>",
          "PK<DockerImage<Domain<'Object Detection'>, Framework<'tensorflow', '1.14'>>>",
          "Name<DockerImage<Domain<'Object Detection'>, Framework<'tensorflow', '1.14'>>>"
        ],
        "help": "The docker image to use"
      },
      "num_train_steps": {
        "types": ["int"],
        "default": 50000,
        "help": "The number of training steps to perform."
      },
      "min_dimension": {
        "types": ["int"],
        "default": 800,
        "help": "The minimum dimension (width or height) for images."
      },
      "max_dimension": {
        "types": ["int"],
        "default": 1365,
        "help": "The maximum dimension (width or height) for images."
      },
      "pretrained_model": {
        "types": [
          "Name<PretrainedModel<Domain<'Object Detection'>, Framework<'tensorflow', '1.14'>>>",
          "PK<PretrainedModel<Domain<'Object Detection'>, Framework<'tensorflow', '1.14'>>>"
        ],
        "default": "mask_rcnn_inception_v2_coco_2018_01_28",
        "help": "The pretrained Mask R-CNN Inception v2 model to use."
      },
      "body": {
        "types": ["Array<str>", "str"],
        "default": [
          "model {",
          "  faster_rcnn {",
          "    number_of_stages: 3",
          "    num_classes: ${num-classes}",
          "    image_resizer {",
          "      keep_aspect_ratio_resizer {",
          "        min_dimension: ${min-dimension}",
          "        max_dimension: ${max-dimension}",
          "      }",
          "    }",
          "    feature_extractor {",
          "      type: \"faster_rcnn_inception_v2\"",
          "      first_stage_features_stride: 16",
          "    }",
          "    first_stage_anchor_generator {",
          "      grid_anchor_generator {",
          "        height_stride: 16",
          "        width_stride: 16",
          "        scales: 0.25",
          "        scales: 0.5",
          "        scales: 1.0",
          "        scales: 2.0",
          "        aspect_ratios: 0.5",
          "        aspect_ratios: 1.0",
          "        aspect_ratios: 2.0",
          "      }",
          "    }",
          "    first_stage_box_predictor_conv_hyperparams {",
          "      op: CONV",
          "      regularizer {",
          "        l2_regularizer {",
          "          weight: 0.0",
          "        }",
          "      }",
          "      initializer {",
          "        truncated_normal_initializer {",
          "          stddev: 0.00999999977648",
          "        }",
          "      }",
          "    }",
          "    first_stage_nms_score_threshold: 0.0",
          "    first_stage_nms_iou_threshold: 0.699999988079",
          "    first_stage_max_proposals: 100",
          "    first_stage_localization_loss_weight: 2.0",
          "    first_stage_objectness_loss_weight: 1.0",
          "    initial_crop_size: 14",
          "    maxpool_kernel_size: 2",
          "    maxpool_stride: 2",
          "    second_stage_box_predictor {",
          "      mask_rcnn_box_predictor {",
          "        fc_hyperparams {",
          "          op: FC",
          "          regularizer {",
          "            l2_regularizer {",
          "              weight: 0.0",
          "            }",
          "          }",
          "          initializer {",
          "            variance_scaling_initializer {",
          "              factor: 1.0",
          "              uniform: true",
          "              mode: FAN_AVG",
          "            }",
          "          }",
          "        }",
          "        use_dropout: false",
          "        dropout_keep_probability: 1.0",
          "        conv_hyperparams {",
          "          op: CONV",
          "          regularizer {",
          "            l2_regularizer {",
          "              weight: 0.0",
          "            }",
          "          }",
          "          initializer {",
          "            truncated_normal_initializer {",
          "              stddev: 0.00999999977648",
          "            }",
          "          }",
          "        }",
          "        predict_instance_masks: true",
          "        mask_prediction_conv_depth: 0",
          "        mask_height: 15",
          "        mask_width: 15",
          "        mask_prediction_num_conv_layers: 2",
          "      }",
          "    }",
          "    second_stage_post_processing {",
          "      batch_non_max_suppression {",
          "        score_threshold: 0.300000011921",
          "        iou_threshold: 0.600000023842",
          "        max_detections_per_class: 100",
          "        max_total_detections: 100",
          "      }",
          "      score_converter: SOFTMAX",
          "    }",
          "    second_stage_localization_loss_weight: 2.0",
          "    second_stage_classification_loss_weight: 1.0",
          "    second_stage_mask_prediction_loss_weight: 4.0",
          "  }",
          "}",
          "train_config {",
          "  batch_size: 1",
          "  data_augmentation_options {",
          "    random_horizontal_flip {",
          "    }",
          "  }",
          "  optimizer {",
          "    momentum_optimizer {",
          "      learning_rate {",
          "        manual_step_learning_rate {",
          "          initial_learning_rate: 0.000199999994948",
          "          schedule {",
          "            step: 900000",
          "            learning_rate: 1.99999994948e-05",
          "          }",
          "          schedule {",
          "            step: 1200000",
          "            learning_rate: 1.99999999495e-06",
          "          }",
          "        }",
          "      }",
          "      momentum_optimizer_value: 0.899999976158",
          "    }",
          "    use_moving_average: false",
          "  }",
          "  gradient_clipping_by_norm: 10.0",
          "  fine_tune_checkpoint: \"/models/pretrainedmodel/model.ckpt\"",
          "  from_detection_checkpoint: true",
          "  num_steps: 200000",
          "}",
          "train_input_reader {",
          "  label_map_path: \"/data//train/labels.pbtxt\"",
          "  load_instance_masks: true",
          "  mask_type: PNG_MASKS",
          "  tf_record_input_reader {",
          "    input_path: \"/data/train/data.tfrecords\"",
          "  }",
          "}",
          "eval_config {",
          "  num_examples: 8000",
          "  max_evals: 10",
          "  use_moving_averages: false",
          "}",
          "eval_input_reader {",
          "  label_map_path: \"/data/test/labels.pbtxt\"",
          "  shuffle: false",
          "  num_readers: 1",
          "  load_instance_masks: true",
          "  mask_type: PNG_MASKS",
          "  tf_record_input_reader {",
          "    input_path: \"/data/test/data.tfrecords\"",
          "  }",
          "}",
          ""
        ],
        "help": "The body of the function"
      }
    }
  }
}
